---
title: "Data Science Homework 5"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

### Problem 1

*Importing and tidying data*

```{r importing and tidying data, message = FALSE}
file_names <- list.files("./data/", full.names = TRUE) # getting file names and path

# reading all files into a list called df
df <- file_names %>% 
  map(read_csv) 

# adding study_arm and study_id
for (i in 1:20) {
  if (i < 11) {
    df[[i]] <- df[[i]] %>% 
      mutate(arm = "Control", 
             study_id = i)
  } else if (i > 10) {
    df[[i]] <- df[[i]] %>% 
      mutate(arm = "Experimental", 
             study_id = i - 10)
  }
}

# tidying
df <- bind_rows(df) %>% 
  gather(key = week, value = obs, week_1:week_8) %>% 
  arrange(week, study_id) %>% 
  separate(col = week, into = c("delete", "week")) %>% 
  select(-delete)
```

To import and clean the 20 separate files provided, I first used `list.files` with the `full.names` option set to ` TRUE` so I had a list containing the file names and relative paths for each file. I then used `map` to iterate over the file names and saved the resulting list as an object named `df`. I purposely used `map` instead of `map_df` because I wanted to keep all of the observations as separated items in a list while I added the study arm variable as well as the study ID variable. 

In order to create the two new variables I had two options: I could write a function and then use `map` to iterate over that function or I could forgoe the function and just use a for-loop. I chose the latter as, *in this case*, the for-loop seemed the most straight forward path. Because `list.files` stores the file names in alphabetical and then numerical order I used conditional logic with the list indexes so that the first 10 items in the list were assigned the "control" level for the study arm  variable and that the last 10 items in the list were assigned the "experimental" level. For some odd reason, study ID is not a unique identifier in this data and repeats across study arm. Thus, for the last 10 items in the list I subtracted 10 from the index and assigned that number as the study ID for that participant. 

After the for-loop, I stacked all the items in the list and then tidied the resulting data frame by transforming it from wide to long form.

*Creating spaghetti plot*

```{r spaghetti plot, fig.align = "center", out.width = "75%", dpi = 300}
# spaghetti plot...
df %>% 
  mutate(week = as.double(week), 
         study_id = as.character(study_id)) %>% 
  group_by(arm, study_id) %>% 
  ggplot(aes(x = week, y = obs, color = arm, type = study_id)) + 
    geom_line() + 
  theme_bw() + 
  labs(title = "Trends in observations across study period for each\n participant stratified by study arm",
       x = "Week", 
       y = "Observation") + 
  viridis::scale_color_viridis(name = "Study arm",
                               discrete = TRUE) + 
  theme(legend.position = "bottom", 
        legend.direction = "horizontal", 
        legend.box.spacing = unit(0.05, "cm"),
        plot.title = element_text(size = 11))
```

The spaghetti plot shows that individuals assigned to the experimental arm appear to have increasing observation values across the study period while individuals assigned to the control arm appear to remain constant. We could fit a mixed-effects model with a random intercept to test this. 


### Problem 2

```{r importing data, message = FALSE}
# importing data from github link
murder <- 
  read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>% 
  janitor::clean_names()
```

The Washington Post homicide dataset contains information on `r nrow(murder)` homicides across 50 major U.S. cities from 2007 to 2015. There are `r ncol(murder)` variables. Variables provide information on the reported date of the homicide, the victims name, age, and whether or not the case was solved. 

```{r creating dataset with unsolved and total}
murder <- murder %>% 
  unite("city_state", c("city", "state"), sep = ", ")

# unsolved cases in a city
unsolved <- murder %>% 
  group_by(city_state) %>% 
  filter(disposition %in% c("Closed without arrest", 
                            "Open/No arrest")) %>% 
  summarize(unsolved = n()) 

# total cases in a city
total_cases <- murder %>% 
  group_by(city_state) %>%
  summarize(total = n())

# joining previously created data sets
all_cases <- left_join(unsolved, total_cases, by = "city_state")
```

The create a summary data set that contained the number of total cases and the number of unsolved cases in each city I first split the dataset into two separate datasets: `unsolved` which contains the number of unsolved cases in each city and `total` which contains the total number of cases in each city. I then joined the two datasets on the "city_state" variable and saved the resulting data set as `all_cases`

```{r echo = FALSE, fig.align = "center", fig.height = 8, out.width = "80%", dpi = 300}
# this provides more info but not needed

# unsolved %>% 
#   mutate(city_state = fct_reorder(city_state, unsolved)) %>% 
#   ggplot(aes(x = city_state, y = unsolved)) + 
#   geom_bar(stat = "identity", fill = "#A35E60") + 
#   coord_flip() + 
#   labs(title = "Number of unsolved homicides in 50 major US cities", 
#        y = "Number of unsolved homicides", 
#        x = "City") + 
#   theme_bw() + 
#   theme(plot.title = element_text(size = 12))
```

```{r prop_unsolved function}
prop_unsolved <- function(df) {

  # uses unsolved and total columns in data set
  ci_unsolved <- prop.test(df$unsolved, df$total)
  
  # putting estimate and bounds in a nice data set
  broom::tidy(ci_unsolved) %>% 
    select(estimate, conf.low, conf.high)
}
```

To obtain the estimated proportion of unsolved cases and corresponding confidence interval for a city I wrote a function called `prop_unsolved `that takes one input, the name of the data set. The function then runs `prop.test` using the "unsolved" and "total" columns in the corresponding data. This result is saved to an object called `ci_unsolved` which is then cleaned up using `tidy` from the broom packaged. 

```{r prop_unsolved for baltimore}
all_cases %>% 
  filter(city_state == "Baltimore, MD") %>% 
  prop_unsolved() %>% 
  mutate(estimate = round(estimate, 3), 
         conf.low = round(conf.low, 3), 
         conf.high = round(conf.high, 3)) %>% 
  rename(Estimate = estimate, 
         "Lower bound" = conf.low, 
         "Upper bound" = conf.high) %>% 
  knitr::kable()
```

The above table shows the estimated proportion and confidence interval of unsolved homicide cases in Baltimore. I estimate that 64.6% of homicide cases in Baltimore are unsolved (95% CI: 62.8%, 66.3%).

```{r prop_unsolved iteration over all cities, fig.align = "center", out.width = "75%", fig.height = 7, dpi = 300}
# nesting all cases
city_nest <- nest(all_cases, unsolved:total)

# prop_unsolved(city_nest$data[[1]])
# map(city_nest$data, prop_unsolved)

# iterating prop_unsolved over list column 'data' for each city
unsolved_ci <- city_nest %>% 
  mutate(prop_unsolved = map(data, prop_unsolved)) %>% 
  unnest() %>% 
  rename(conf_low = conf.low, 
         conf_high = conf.high)

# making a plot
unsolved_ci %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point(color = "#A35E60") + 
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) + 
  coord_flip() + 
  labs(title = "Proportion of unsolved cases in 50 major US cities", 
       y = "Proportion of unsolved cases", 
       x = "City", 
       caption = "Error bars represent 95% confidence interval") + 
  theme_classic() 
```

To iterate over all cities using the function I created, I used `nest` to create a data set called `city_nest` that has a column named data that is a list column containing the number of unsolved and total cases in each city. I then used `map` within a `mutate` function to iterate over the data column and created a new list column that contains the estimate and confidence inteval. I then plotted the estimates. 

The resulting plot shows the estimated proportion of unsolved homicide cases in each of the 50 cities with a corresponding confidence interval. Chicago has the highest proportion of unsolved cases, followed by New Orleans and Baltimore. Richmond has the lowest proportion of unsolved cases. 















